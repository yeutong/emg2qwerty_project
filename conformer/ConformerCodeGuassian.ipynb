{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: unidecode in c:\\users\\aisha\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (1.3.8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
      "[notice] To update, run: C:\\Users\\aisha\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install unidecode\n",
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created train_loader with 30713 windows\n",
      "Created val_loader with 1698 windows\n",
      "Created test_loader with 2251 windows\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, ConcatDataset, Subset\n",
    "from emg2qwerty.data import WindowedEMGDataset\n",
    "from emg2qwerty.transforms import Compose, ToTensor, LogSpectrogram, SpecAugment, RandomBandRotation, TemporalAlignmentJitter, GaussianNoise\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "num_workers = 2\n",
    "\n",
    "# 1. Load the configuration file\n",
    "config_path = Path(\"config/user/single_user.yaml\")\n",
    "with open(config_path, 'r') as f:\n",
    "    first_line = f.readline()\n",
    "    if first_line.startswith('# @package'):\n",
    "        cfg = yaml.safe_load(f)\n",
    "    else:\n",
    "        f.seek(0)\n",
    "        cfg = yaml.safe_load(f)\n",
    "\n",
    "# 2. Set the data root path\n",
    "data_root = Path(\"data\")\n",
    "\n",
    "# 3. Create transforms\n",
    "# Training transforms with augmentation\n",
    "train_transform = Compose([\n",
    "    ToTensor(),\n",
    "    GaussianNoise(),\n",
    "    LogSpectrogram(),\n",
    "    SpecAugment(),\n",
    "    RandomBandRotation()\n",
    "])\n",
    "\n",
    "# Validation/test transforms without augmentation\n",
    "eval_transform = Compose([\n",
    "    ToTensor(),\n",
    "    LogSpectrogram()\n",
    "])\n",
    "\n",
    "# 4. Create datasets\n",
    "train_datasets = []\n",
    "for session_info in cfg['dataset']['train']:\n",
    "    session_id = session_info['session']\n",
    "    file_path = data_root / f\"{session_id}.hdf5\"\n",
    "\n",
    "    dataset = WindowedEMGDataset(\n",
    "        hdf5_path=file_path,\n",
    "        window_length=2000,  # 1 second at 2kHz\n",
    "        stride=1000,         # 50% overlap\n",
    "        padding=(200, 200),  # 100ms context on each side\n",
    "        jitter=True,         # Apply jitter for training\n",
    "        transform=train_transform\n",
    "    )\n",
    "    train_datasets.append(dataset)\n",
    "\n",
    "val_datasets = []\n",
    "for session_info in cfg['dataset']['val']:\n",
    "    session_id = session_info['session']\n",
    "    file_path = data_root / f\"{session_id}.hdf5\"\n",
    "\n",
    "    dataset = WindowedEMGDataset(\n",
    "        hdf5_path=file_path,\n",
    "        window_length=2000,\n",
    "        stride=1000,\n",
    "        padding=(200, 200),\n",
    "        jitter=False,  # No jitter for validation\n",
    "        transform=eval_transform\n",
    "    )\n",
    "    val_datasets.append(dataset)\n",
    "\n",
    "test_datasets = []\n",
    "for session_info in cfg['dataset']['test']:\n",
    "    session_id = session_info['session']\n",
    "    file_path = data_root / f\"{session_id}.hdf5\"\n",
    "\n",
    "    dataset = WindowedEMGDataset(\n",
    "        hdf5_path=file_path,\n",
    "        window_length=2000,\n",
    "        stride=1000,\n",
    "        padding=(200, 200),\n",
    "        jitter=False,  # No jitter for testing\n",
    "        transform=eval_transform\n",
    "    )\n",
    "    test_datasets.append(dataset)\n",
    "\n",
    "# 5. Combine datasets and create DataLoaders\n",
    "train_dataset = ConcatDataset(train_datasets)\n",
    "val_dataset = ConcatDataset(val_datasets)\n",
    "test_dataset = ConcatDataset(test_datasets)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=WindowedEMGDataset.collate,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,  # No shuffle for validation\n",
    "    collate_fn=WindowedEMGDataset.collate,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,  # No shuffle for testing\n",
    "    collate_fn=WindowedEMGDataset.collate,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "# for debugging\n",
    "small_train_loader = DataLoader(\n",
    "    Subset(train_dataset, list(range(100))),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=WindowedEMGDataset.collate,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "small_val_loader = DataLoader(\n",
    "    Subset(val_dataset, list(range(100))),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,  # No shuffle for validation\n",
    "    collate_fn=WindowedEMGDataset.collate,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "print(f\"Created train_loader with {len(train_dataset)} windows\")\n",
    "print(f\"Created val_loader with {len(val_dataset)} windows\")\n",
    "print(f\"Created test_loader with {len(test_dataset)} windows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        user                                            session  \\\n",
      "0   89335547  2021-06-03-1622765527-keystrokes-dca-study@1-0...   \n",
      "1   89335547  2021-06-02-1622681518-keystrokes-dca-study@1-0...   \n",
      "2   89335547  2021-06-04-1622863166-keystrokes-dca-study@1-0...   \n",
      "3   89335547  2021-07-22-1627003020-keystrokes-dca-study@1-0...   \n",
      "4   89335547  2021-07-21-1626916256-keystrokes-dca-study@1-0...   \n",
      "5   89335547  2021-07-22-1627004019-keystrokes-dca-study@1-0...   \n",
      "6   89335547  2021-06-05-1622885888-keystrokes-dca-study@1-0...   \n",
      "7   89335547  2021-06-02-1622679967-keystrokes-dca-study@1-0...   \n",
      "8   89335547  2021-06-03-1622764398-keystrokes-dca-study@1-0...   \n",
      "9   89335547  2021-07-21-1626917264-keystrokes-dca-study@1-0...   \n",
      "10  89335547  2021-06-05-1622889105-keystrokes-dca-study@1-0...   \n",
      "11  89335547  2021-06-03-1622766673-keystrokes-dca-study@1-0...   \n",
      "12  89335547  2021-06-04-1622861066-keystrokes-dca-study@1-0...   \n",
      "13  89335547  2021-07-22-1627001995-keystrokes-dca-study@1-0...   \n",
      "14  89335547  2021-06-05-1622884635-keystrokes-dca-study@1-0...   \n",
      "15  89335547  2021-07-21-1626915176-keystrokes-dca-study@1-0...   \n",
      "16  89335547  2021-06-04-1622862148-keystrokes-dca-study@1-0...   \n",
      "17  89335547  2021-06-02-1622682789-keystrokes-dca-study@1-0...   \n",
      "\n",
      "    duration_mins  duration_hours  num_keystrokes  num_prompts  split  \n",
      "0       16.055175        0.267586            4667          154  train  \n",
      "1       18.069472        0.301158            4591          154  train  \n",
      "2       14.921639        0.248694            4695          154  train  \n",
      "3       15.145979        0.252433            4438          154  train  \n",
      "4       15.131067        0.252184            4504          154  train  \n",
      "5       14.922270        0.248705            4432          154  train  \n",
      "6       17.357376        0.289290            4636          154  train  \n",
      "7       19.807583        0.330126            4690          154  train  \n",
      "8       15.610410        0.260173            4532          154  train  \n",
      "9       14.662873        0.244381            4450          154  train  \n",
      "10      16.961649        0.282694            4558          154  train  \n",
      "11      15.617070        0.260285            4480          154  train  \n",
      "12      14.928824        0.248814            4569          154  train  \n",
      "13      14.546556        0.242443            4383          154  train  \n",
      "14      17.746499        0.295775            4647          154  train  \n",
      "15      14.518713        0.241979            4375          154  train  \n",
      "16      14.165903        0.236098            4524          153    val  \n",
      "17      18.768094        0.312802            4627          154   test  \n",
      "Overall statistics:\n",
      "Number of sessions: 18\n",
      "Total duration (hours): 4.82\n",
      "Total keystrokes: 81798\n",
      "Total prompts: 2771\n",
      "\n",
      "Statistics by split:\n",
      "       count  duration_hours  num_keystrokes  num_prompts\n",
      "split                                                    \n",
      "test       1        0.312802            4627          154\n",
      "train     16        4.266719           72647         2464\n",
      "val        1        0.236098            4524          153\n",
      "\n",
      "Train set statistics:\n",
      "Number of sessions: 16\n",
      "Total duration (hours): 4.27\n",
      "Total keystrokes: 72647\n",
      "Total prompts: 2464\n",
      "\n",
      "Val set statistics:\n",
      "Number of sessions: 1\n",
      "Total duration (hours): 0.24\n",
      "Total keystrokes: 4524\n",
      "Total prompts: 153\n",
      "\n",
      "Test set statistics:\n",
      "Number of sessions: 1\n",
      "Total duration (hours): 0.31\n",
      "Total keystrokes: 4627\n",
      "Total prompts: 154\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import h5py\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import json\n",
    "\n",
    "# Load the configuration file\n",
    "config_path = Path(\"C:/Users/aisha/OneDrive/Documents/cs247_project/config/user/single_user.yaml\")\n",
    "with open(config_path, 'r') as f:\n",
    "    # Skip the first line which contains \"# @package _global_\"\n",
    "    first_line = f.readline()\n",
    "    if first_line.startswith('# @package'):\n",
    "        cfg = yaml.safe_load(f)\n",
    "    else:\n",
    "        # If the first line doesn't have the package declaration, reset to beginning of file\n",
    "        f.seek(0)\n",
    "        cfg = yaml.safe_load(f)\n",
    "\n",
    "# Set the data root path (adjust this to your actual data location)\n",
    "data_root = Path(\"C:/Users/aisha/OneDrive/Documents/cs247_project/data\")  # Update this to your actual data path\n",
    "\n",
    "# Function to extract metadata from a session file\n",
    "def extract_session_metadata(file_path):\n",
    "    try:\n",
    "        with h5py.File(file_path, 'r') as f:\n",
    "            emg2qwerty_group = f['emg2qwerty']\n",
    "\n",
    "            # Get metadata attributes\n",
    "            metadata = {}\n",
    "            for key, val in emg2qwerty_group.attrs.items():\n",
    "                if key in ['keystrokes', 'prompts']:\n",
    "                    try:\n",
    "                        metadata[key] = json.loads(val)\n",
    "                    except json.JSONDecodeError:\n",
    "                        # Fall back to yaml if json fails\n",
    "                        metadata[key] = yaml.safe_load(val)\n",
    "                else:\n",
    "                    metadata[key] = val\n",
    "\n",
    "            # Calculate basic statistics\n",
    "            session_stats = {\n",
    "                'user': metadata.get('user', 'unknown'),\n",
    "                'session': metadata.get('session_name', 'unknown'),\n",
    "                'duration_mins': metadata.get('duration_mins', 0),\n",
    "                'duration_hours': metadata.get('duration_mins', 0) / 60.0,\n",
    "                'num_keystrokes': len(metadata.get('keystrokes', [])),\n",
    "                'num_prompts': len(metadata.get('prompts', [])),\n",
    "                'split': 'unknown'  # Will be set later\n",
    "            }\n",
    "\n",
    "            return session_stats\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Collect all sessions from config\n",
    "all_sessions = []\n",
    "splits = ['train', 'val', 'test']\n",
    "\n",
    "for split in splits:\n",
    "    if split in cfg['dataset']:\n",
    "        for session_info in cfg['dataset'][split]:\n",
    "            session_info_dict = {\n",
    "                'user': session_info['user'],\n",
    "                'session': session_info['session'],\n",
    "                'split': split\n",
    "            }\n",
    "            all_sessions.append(session_info_dict)\n",
    "\n",
    "# Process each session to get metadata\n",
    "session_stats = []\n",
    "\n",
    "for session_info in all_sessions:\n",
    "    user_id = session_info['user']\n",
    "    session_id = session_info['session']\n",
    "    split = session_info['split']\n",
    "    file_path = data_root / f\"{session_id}.hdf5\"\n",
    "\n",
    "    if file_path.exists():\n",
    "        stats = extract_session_metadata(file_path)\n",
    "        if stats:\n",
    "            stats['split'] = split  # Add which split this session belongs to\n",
    "            session_stats.append(stats)\n",
    "    else:\n",
    "        print(f\"Warning: File not found: {file_path}\")\n",
    "\n",
    "# Create a DataFrame from the collected statistics\n",
    "df = pd.DataFrame(session_stats)\n",
    "print(df)\n",
    "\n",
    "# Calculate aggregated statistics per split\n",
    "split_stats = df.groupby('split').agg({\n",
    "    'session': 'count',\n",
    "    'duration_hours': 'sum',\n",
    "    'num_keystrokes': 'sum',\n",
    "    'num_prompts': 'sum'\n",
    "}).rename(columns={'session': 'count'})\n",
    "\n",
    "# Calculate overall statistics\n",
    "num_sessions = len(df)\n",
    "total_duration_hours = df[\"duration_hours\"].sum()\n",
    "total_keystrokes = df[\"num_keystrokes\"].sum()\n",
    "total_prompts = df[\"num_prompts\"].sum()\n",
    "\n",
    "# Print the results\n",
    "print(\"Overall statistics:\")\n",
    "print(f\"Number of sessions: {num_sessions}\")\n",
    "print(f\"Total duration (hours): {total_duration_hours:.2f}\")\n",
    "print(f\"Total keystrokes: {total_keystrokes}\")\n",
    "print(f\"Total prompts: {total_prompts}\")\n",
    "\n",
    "print(\"\\nStatistics by split:\")\n",
    "print(split_stats)\n",
    "\n",
    "# Also print individual statistics for train, val, and test sets\n",
    "for split in splits:\n",
    "    if split in df['split'].values:\n",
    "        split_df = df[df['split'] == split]\n",
    "        print(f\"\\n{split.capitalize()} set statistics:\")\n",
    "        print(f\"Number of sessions: {len(split_df)}\")\n",
    "        print(f\"Total duration (hours): {split_df['duration_hours'].sum():.2f}\")\n",
    "        print(f\"Total keystrokes: {split_df['num_keystrokes'].sum()}\")\n",
    "        print(f\"Total prompts: {split_df['num_prompts'].sum()}\")\n",
    "\n",
    "# Save to CSV files\n",
    "df.to_csv(\"all_session_metadata.csv\", index=False)\n",
    "split_stats.to_csv(\"split_statistics.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from emg2qwerty.charset import charset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        # Create position encodings\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is [seq_len, batch, features]\n",
    "        return x + self.pe[:x.size(0), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvModule(nn.Module):\n",
    "    def __init__(self, d_model, kernel_size=15, expansion_factor=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        # Pointwise Conv -> Depthwise Conv -> Pointwise Conv\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "        self.pointwise_conv1 = nn.Conv1d(d_model, d_model * expansion_factor, kernel_size=1)\n",
    "        self.glu = nn.GLU(dim=1)\n",
    "        self.depthwise_conv = nn.Conv1d(\n",
    "            d_model, d_model,\n",
    "            kernel_size=kernel_size,\n",
    "            padding=(kernel_size-1)//2,\n",
    "            groups=d_model\n",
    "        )\n",
    "        self.batch_norm = nn.BatchNorm1d(d_model)\n",
    "        self.swish = nn.SiLU()\n",
    "        self.pointwise_conv2 = nn.Conv1d(d_model, d_model, kernel_size=1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is [batch, seq_len, d_model]\n",
    "        residual = x\n",
    "\n",
    "        # Layer norm\n",
    "        x = self.layer_norm(x)\n",
    "\n",
    "        # Transpose to [batch, d_model, seq_len] for Conv1d\n",
    "        x = x.transpose(1, 2)\n",
    "\n",
    "        # Pointwise conv and GLU activation\n",
    "        x = self.pointwise_conv1(x)\n",
    "        x = self.glu(x)\n",
    "\n",
    "        # Depthwise conv\n",
    "        x = self.depthwise_conv(x)\n",
    "        x = self.batch_norm(x)\n",
    "        x = self.swish(x)\n",
    "\n",
    "        # Pointwise conv\n",
    "        x = self.pointwise_conv2(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Transpose back to [batch, seq_len, d_model]\n",
    "        x = x.transpose(1, 2)\n",
    "\n",
    "        # Residual connection\n",
    "        return x + residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardModule(nn.Module):\n",
    "    def __init__(self, d_model, expansion_factor=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "        self.linear1 = nn.Linear(d_model, d_model * expansion_factor)\n",
    "        self.swish = nn.SiLU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(d_model * expansion_factor, d_model)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        x = self.layer_norm(x)\n",
    "        x = self.linear1(x)\n",
    "        x = self.swish(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        return x + residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, nhead=8, dropout=0.1, kernel_size=15):\n",
    "        super().__init__()\n",
    "\n",
    "        # First feed-forward module (half-step)\n",
    "        self.feed_forward1 = FeedForwardModule(d_model, dropout=dropout)\n",
    "\n",
    "        # Multi-headed self-attention\n",
    "        self.self_attn_layer_norm = nn.LayerNorm(d_model)\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=False)\n",
    "        self.self_attn_dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Convolution module\n",
    "        self.conv_module = ConvModule(d_model, kernel_size=kernel_size, dropout=dropout)\n",
    "\n",
    "        # Second feed-forward module (half-step)\n",
    "        self.feed_forward2 = FeedForwardModule(d_model, dropout=dropout)\n",
    "\n",
    "        # Final layer norm\n",
    "        self.final_layer_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: input tensor of shape [seq_len, batch, d_model]\n",
    "            attention_mask: mask for self-attention of shape [batch, seq_len]\n",
    "        \"\"\"\n",
    "        # Apply first FFN (half step)\n",
    "        x = x + 0.5 * self.feed_forward1(x)\n",
    "\n",
    "        # Apply self-attention\n",
    "        residual = x\n",
    "        x = self.self_attn_layer_norm(x)\n",
    "\n",
    "        # For PyTorch's MultiheadAttention when batch_first=False:\n",
    "        # x shape should be [seq_len, batch, d_model]\n",
    "        # key_padding_mask should be [batch, seq_len]\n",
    "        attn_output, _ = self.self_attn(\n",
    "            x, x, x,\n",
    "            key_padding_mask=attention_mask  # Already in correct shape [batch, seq_len]\n",
    "        )\n",
    "\n",
    "        x = residual + self.self_attn_dropout(attn_output)\n",
    "\n",
    "        # Apply convolution module\n",
    "        residual = x\n",
    "        x_t = x.transpose(0, 1)  # [batch, seq_len, d_model]\n",
    "        x_t = self.conv_module(x_t)\n",
    "        x = residual + x_t.transpose(0, 1)  # [seq_len, batch, d_model]\n",
    "\n",
    "        # Apply second FFN (half step)\n",
    "        x = x + 0.5 * self.feed_forward2(x)\n",
    "\n",
    "        # Final layer norm\n",
    "        x = self.final_layer_norm(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EMGConformer(nn.Module):\n",
    "    def __init__(self, num_classes=99, d_model=256, nhead=8, num_layers=4, dropout=0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Input EMG signal dimensions\n",
    "        self.features = 1056  # 2 bands * 16 channels * 33 frequencies\n",
    "        \n",
    "        # CNN feature extraction with output channels = cnn_output_dim\n",
    "        cnn_output_dim = 256  # This should match your last CNN layer's output channels\n",
    "        \n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv1d(self.features, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            \n",
    "            nn.Conv1d(512, cnn_output_dim, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm1d(cnn_output_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        # Add projection layer to ensure dimensions match\n",
    "        self.projection = nn.Linear(cnn_output_dim, d_model) if cnn_output_dim != d_model else nn.Identity()\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "        \n",
    "        # Conformer blocks\n",
    "        self.conformer_blocks = nn.ModuleList([\n",
    "            ConformerBlock(d_model, nhead, dropout, kernel_size=15)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Linear(d_model, num_classes)\n",
    "    \n",
    "    def forward(self, x, input_lengths=None):\n",
    "        # x shape: [T, N, B, C, F] - [time, batch, bands, channels, freq]\n",
    "        T, N, B, C, F = x.shape\n",
    "        device = x.device\n",
    "        \n",
    "        # Flatten features and prepare for Conv1d\n",
    "        x = x.reshape(T, N, B*C*F).permute(1, 2, 0)  # [N, B*C*F, T]\n",
    "        \n",
    "        # Apply CNN layers\n",
    "        x = self.conv_layers(x)  # [N, cnn_output_dim, T//2]\n",
    "        \n",
    "        # Calculate new sequence lengths after CNN pooling \n",
    "        if input_lengths is not None:\n",
    "            new_lengths = torch.div(input_lengths, 2, rounding_mode='floor')\n",
    "            new_lengths = torch.clamp(new_lengths, min=1)\n",
    "            # Create padding mask for transformer\n",
    "            max_len = x.size(2)\n",
    "            padding_mask = (torch.arange(max_len, device=device).expand(N, max_len) \n",
    "                           >= new_lengths.unsqueeze(1))\n",
    "        else:\n",
    "            padding_mask = None\n",
    "        \n",
    "        # Prepare for conformer: [T//2, N, cnn_output_dim]\n",
    "        x = x.permute(2, 0, 1)\n",
    "        \n",
    "        # Apply projection if needed\n",
    "        x = self.projection(x)  # [T//2, N, d_model]\n",
    "        \n",
    "        # Apply positional encoding\n",
    "        x = self.pos_encoder(x)\n",
    "        \n",
    "        # Apply conformer blocks\n",
    "        for block in self.conformer_blocks:\n",
    "            x = block(x, padding_mask)\n",
    "        \n",
    "        # Apply classifier\n",
    "        x = self.classifier(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cer(predictions, targets):\n",
    "    \"\"\"\n",
    "    Calculate Character Error Rate\n",
    "    \"\"\"\n",
    "    total_edits = 0\n",
    "    total_length = 0\n",
    "\n",
    "    for pred, target in zip(predictions, targets):\n",
    "        # Remove blank tokens and duplicates\n",
    "        filtered_pred = []\n",
    "        prev = None\n",
    "        for p in pred:\n",
    "            if p != charset().null_class and p != prev:\n",
    "                filtered_pred.append(p)\n",
    "            prev = p\n",
    "\n",
    "        # Calculate edit distance\n",
    "        edit_distance = levenshtein_distance(filtered_pred, target)\n",
    "\n",
    "        total_edits += edit_distance\n",
    "        total_length += len(target)\n",
    "\n",
    "    # Return CER\n",
    "    return total_edits / max(1, total_length)\n",
    "\n",
    "def levenshtein_distance(s1, s2):\n",
    "    \"\"\"\n",
    "    Calculate Levenshtein distance between two sequences\n",
    "    \"\"\"\n",
    "    if len(s1) < len(s2):\n",
    "        return levenshtein_distance(s2, s1)\n",
    "\n",
    "    if len(s2) == 0:\n",
    "        return len(s1)\n",
    "\n",
    "    previous_row = range(len(s2) + 1)\n",
    "    for i, c1 in enumerate(s1):\n",
    "        current_row = [i + 1]\n",
    "        for j, c2 in enumerate(s2):\n",
    "            insertions = previous_row[j + 1] + 1\n",
    "            deletions = current_row[j] + 1\n",
    "            substitutions = previous_row[j] + (c1 != c2)\n",
    "            current_row.append(min(insertions, deletions, substitutions))\n",
    "        previous_row = current_row\n",
    "\n",
    "    return previous_row[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_predictions(log_probs, input_lengths):\n",
    "    \"\"\"Perform CTC greedy decoding on log probabilities\"\"\"\n",
    "    predictions = log_probs.argmax(dim=2)\n",
    "    decoded = []\n",
    "    \n",
    "    for i in range(predictions.size(1)):  # Loop through batch\n",
    "        pred_len = input_lengths[i].item()\n",
    "        pred = predictions[:pred_len, i].tolist()\n",
    "        \n",
    "        # Remove consecutive duplicates and blanks (CTC decoding)\n",
    "        result = []\n",
    "        prev = None\n",
    "        for p in pred:\n",
    "            if p != 0:  # Not blank\n",
    "                if p != prev:  # Not a duplicate\n",
    "                    result.append(p)\n",
    "            prev = p\n",
    "        \n",
    "        decoded.append(result)\n",
    "    \n",
    "    return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, device, epochs=100, lr=0.001, weight_decay=1e-4):\n",
    "    \"\"\"\n",
    "    Train the Conformer model on EMG data\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "\n",
    "    # CTC loss for sequence prediction\n",
    "    criterion = nn.CTCLoss(blank=0, reduction='mean', zero_infinity=True)\n",
    "\n",
    "    # Adam optimizer with improved parameters\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay, betas=(0.9, 0.999), eps=1e-8)\n",
    "\n",
    "    # Learning rate scheduler with warmup\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer,\n",
    "        max_lr=0.002,  # Lower max learning rate\n",
    "        steps_per_epoch=len(train_loader),\n",
    "        epochs=epochs,\n",
    "        pct_start=0.1,  # 10% warmup\n",
    "        div_factor=10,  # Initial lr = max_lr/div_factor\n",
    "        final_div_factor=100  # Final lr = initial_lr/final_div_factor\n",
    "    )\n",
    "\n",
    "    # Add these variables at the beginning of the function:\n",
    "    best_val_cer = float('inf')\n",
    "    best_epoch = 0\n",
    "    \n",
    "    # Training history\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'val_cer': [],\n",
    "        'all_predictions': [],\n",
    "        'all_targets': []\n",
    "    }\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_batches = 0\n",
    "\n",
    "        progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs} [Train]')\n",
    "        for batch_idx, batch in enumerate(progress_bar):\n",
    "            inputs = batch['inputs'].to(device)\n",
    "            targets = batch['targets'].to(device)\n",
    "            input_lengths = batch['input_lengths'].to(device)\n",
    "            target_lengths = batch['target_lengths'].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs, input_lengths)\n",
    "\n",
    "            # Compute log probabilities\n",
    "            log_probs = outputs.log_softmax(2)\n",
    "\n",
    "            # Adjust input lengths for downsampling in the model\n",
    "            # We only have one pooling layer with stride 2\n",
    "            input_lengths = torch.div(input_lengths, 2, rounding_mode='floor')\n",
    "            input_lengths = torch.clamp(input_lengths, min=1)  # Ensure no zero lengths\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(log_probs, targets.T, input_lengths, target_lengths)\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient clipping to prevent exploding gradients\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Update statistics\n",
    "            train_loss += loss.item()\n",
    "            train_batches += 1\n",
    "\n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix({'loss': loss.item()})\n",
    "\n",
    "        avg_train_loss = train_loss / train_batches\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_batches = 0\n",
    "        all_predictions = []\n",
    "        all_targets = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            progress_bar = tqdm(val_loader, desc=f'Epoch {epoch+1}/{epochs} [Val]')\n",
    "            for batch_idx, batch in enumerate(progress_bar):\n",
    "                inputs = batch['inputs'].to(device)\n",
    "                targets = batch['targets'].to(device)\n",
    "                input_lengths = batch['input_lengths'].to(device)\n",
    "                target_lengths = batch['target_lengths'].to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(inputs, input_lengths)\n",
    "\n",
    "                # Compute log probabilities\n",
    "                log_probs = outputs.log_softmax(2)\n",
    "\n",
    "                # Adjust input lengths for downsampling in the model\n",
    "                input_lengths = torch.div(input_lengths, 2, rounding_mode='floor')\n",
    "                input_lengths = torch.clamp(input_lengths, min=1)  # Ensure no zero lengths\n",
    "\n",
    "                # Compute loss\n",
    "                loss = criterion(log_probs, targets.T, input_lengths, target_lengths)\n",
    "\n",
    "                # Update statistics\n",
    "                val_loss += loss.item()\n",
    "                val_batches += 1\n",
    "\n",
    "                # Decode predictions using proper CTC decoding\n",
    "                predictions = decode_predictions(log_probs.cpu(), input_lengths.cpu())\n",
    "                targets_np = targets.cpu().numpy()\n",
    "\n",
    "                # Store predictions and targets for CER calculation\n",
    "                for i in range(inputs.size(1)):  # Loop through batch\n",
    "                    target_seq = targets_np[:target_lengths[i].item(), i]\n",
    "                    all_predictions.append(predictions[i])  # Already processed by decode_predictions\n",
    "                    all_targets.append(target_seq)\n",
    "\n",
    "                # Update progress bar\n",
    "                progress_bar.set_postfix({'loss': loss.item()})\n",
    "\n",
    "        avg_val_loss = val_loss / val_batches\n",
    "        history['val_loss'].append(avg_val_loss)\n",
    "\n",
    "        # Calculate Character Error Rate\n",
    "        cer = calculate_cer(all_predictions, all_targets)\n",
    "        history['val_cer'].append(cer)\n",
    "        history['all_predictions'].append(all_predictions)\n",
    "        history['all_targets'].append(all_targets)\n",
    "\n",
    "        # Add checkpoint saving here:\n",
    "        if cer < best_val_cer:\n",
    "            best_val_cer = cer\n",
    "            best_epoch = epoch + 1\n",
    "            torch.save(model.state_dict(), 'conformer_best.pth')\n",
    "            print(f\"Saved checkpoint at epoch {epoch+1} with CER: {best_val_cer:.4f}\")\n",
    "\n",
    "        # Print epoch summary\n",
    "        print(f'Epoch {epoch+1}/{epochs}:')\n",
    "        print(f'  Train Loss: {avg_train_loss:.4f}')\n",
    "        print(f'  Val Loss: {avg_val_loss:.4f}, Val CER: {cer:.4f}')\n",
    "\n",
    "    return history, best_epoch, best_val_cer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "EMGConformer(\n",
      "  (conv_layers): Sequential(\n",
      "    (0): Conv1d(1056, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.3, inplace=False)\n",
      "    (4): Conv1d(512, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU()\n",
      "    (7): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (8): Dropout(p=0.3, inplace=False)\n",
      "  )\n",
      "  (projection): Identity()\n",
      "  (pos_encoder): PositionalEncoding()\n",
      "  (conformer_blocks): ModuleList(\n",
      "    (0-3): 4 x ConformerBlock(\n",
      "      (feed_forward1): FeedForwardModule(\n",
      "        (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "        (swish): SiLU()\n",
      "        (dropout1): Dropout(p=0.3, inplace=False)\n",
      "        (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "        (dropout2): Dropout(p=0.3, inplace=False)\n",
      "      )\n",
      "      (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "      (self_attn_dropout): Dropout(p=0.3, inplace=False)\n",
      "      (conv_module): ConvModule(\n",
      "        (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "        (glu): GLU(dim=1)\n",
      "        (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "        (batch_norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (swish): SiLU()\n",
      "        (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "        (dropout): Dropout(p=0.3, inplace=False)\n",
      "      )\n",
      "      (feed_forward2): FeedForwardModule(\n",
      "        (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "        (swish): SiLU()\n",
      "        (dropout1): Dropout(p=0.3, inplace=False)\n",
      "        (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "        (dropout2): Dropout(p=0.3, inplace=False)\n",
      "      )\n",
      "      (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (classifier): Linear(in_features=256, out_features=99, bias=True)\n",
      ")\n",
      "Total parameters: 8,118,371\n",
      "Testing model with a small batch...\n",
      "Forward pass successful! Output shape: torch.Size([73, 32, 99])\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/80 [Train]: 100%|██████████| 960/960 [00:47<00:00, 20.07it/s, loss=0.864]\n",
      "Epoch 1/80 [Val]: 100%|██████████| 54/54 [00:04<00:00, 13.03it/s, loss=1.64] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint at epoch 1 with CER: 0.4850\n",
      "Epoch 1/80:\n",
      "  Train Loss: 2.1346\n",
      "  Val Loss: 1.0121, Val CER: 0.4850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/80 [Train]: 100%|██████████| 960/960 [00:44<00:00, 21.43it/s, loss=1.23] \n",
      "Epoch 2/80 [Val]: 100%|██████████| 54/54 [00:04<00:00, 13.29it/s, loss=1.12]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint at epoch 2 with CER: 0.3952\n",
      "Epoch 2/80:\n",
      "  Train Loss: 0.9050\n",
      "  Val Loss: 0.6707, Val CER: 0.3952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/80 [Train]: 100%|██████████| 960/960 [00:44<00:00, 21.44it/s, loss=0.566]\n",
      "Epoch 3/80 [Val]: 100%|██████████| 54/54 [00:04<00:00, 13.09it/s, loss=0.919] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/80:\n",
      "  Train Loss: 0.7826\n",
      "  Val Loss: 0.7134, Val CER: 0.3958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/80 [Train]: 100%|██████████| 960/960 [00:44<00:00, 21.41it/s, loss=0.562]  \n",
      "Epoch 4/80 [Val]: 100%|██████████| 54/54 [00:04<00:00, 13.13it/s, loss=0.965] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/80:\n",
      "  Train Loss: 0.7442\n",
      "  Val Loss: 0.7518, Val CER: 0.3958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/80 [Train]: 100%|██████████| 960/960 [00:44<00:00, 21.44it/s, loss=0.955]  \n",
      "Epoch 5/80 [Val]: 100%|██████████| 54/54 [00:04<00:00, 13.21it/s, loss=0.762]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/80:\n",
      "  Train Loss: 0.7340\n",
      "  Val Loss: 0.7393, Val CER: 0.4201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/80 [Train]: 100%|██████████| 960/960 [00:49<00:00, 19.52it/s, loss=0.736] \n",
      "Epoch 6/80 [Val]: 100%|██████████| 54/54 [00:04<00:00, 13.49it/s, loss=1.11] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint at epoch 6 with CER: 0.3838\n",
      "Epoch 6/80:\n",
      "  Train Loss: 0.6750\n",
      "  Val Loss: 0.6753, Val CER: 0.3838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/80 [Train]: 100%|██████████| 960/960 [00:45<00:00, 21.21it/s, loss=0.27]   \n",
      "Epoch 7/80 [Val]: 100%|██████████| 54/54 [00:03<00:00, 13.71it/s, loss=0.88]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint at epoch 7 with CER: 0.3828\n",
      "Epoch 7/80:\n",
      "  Train Loss: 0.6377\n",
      "  Val Loss: 0.6764, Val CER: 0.3828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/80 [Train]: 100%|██████████| 960/960 [00:43<00:00, 21.85it/s, loss=0.347]   \n",
      "Epoch 8/80 [Val]: 100%|██████████| 54/54 [00:03<00:00, 14.10it/s, loss=1.78]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint at epoch 8 with CER: 0.3726\n",
      "Epoch 8/80:\n",
      "  Train Loss: 0.5999\n",
      "  Val Loss: 0.6034, Val CER: 0.3726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/80 [Train]: 100%|██████████| 960/960 [00:43<00:00, 21.83it/s, loss=0.86]   \n",
      "Epoch 9/80 [Val]: 100%|██████████| 54/54 [00:04<00:00, 13.28it/s, loss=0.827] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint at epoch 9 with CER: 0.3147\n",
      "Epoch 9/80:\n",
      "  Train Loss: 0.5346\n",
      "  Val Loss: 0.4004, Val CER: 0.3147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/80 [Train]: 100%|██████████| 960/960 [00:53<00:00, 18.07it/s, loss=0.508]   \n",
      "Epoch 10/80 [Val]: 100%|██████████| 54/54 [00:04<00:00, 13.37it/s, loss=0.499] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/80:\n",
      "  Train Loss: 0.5153\n",
      "  Val Loss: 0.5733, Val CER: 0.3596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/80 [Train]: 100%|██████████| 960/960 [00:51<00:00, 18.74it/s, loss=0.37]   \n",
      "Epoch 11/80 [Val]: 100%|██████████| 54/54 [00:03<00:00, 13.93it/s, loss=0.598]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/80:\n",
      "  Train Loss: 0.5027\n",
      "  Val Loss: 0.5174, Val CER: 0.3423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/80 [Train]: 100%|██████████| 960/960 [00:50<00:00, 19.06it/s, loss=0.429]  \n",
      "Epoch 12/80 [Val]: 100%|██████████| 54/54 [00:03<00:00, 14.55it/s, loss=0.433] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/80:\n",
      "  Train Loss: 0.5455\n",
      "  Val Loss: 0.5771, Val CER: 0.3672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/80 [Train]: 100%|██████████| 960/960 [00:43<00:00, 21.94it/s, loss=0.41]   \n",
      "Epoch 13/80 [Val]: 100%|██████████| 54/54 [00:04<00:00, 13.35it/s, loss=0.148] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/80:\n",
      "  Train Loss: 0.4999\n",
      "  Val Loss: 0.6329, Val CER: 0.3659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/80 [Train]: 100%|██████████| 960/960 [00:45<00:00, 21.09it/s, loss=0.198]  \n",
      "Epoch 14/80 [Val]: 100%|██████████| 54/54 [00:04<00:00, 13.21it/s, loss=0.802]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/80:\n",
      "  Train Loss: 0.4852\n",
      "  Val Loss: 0.5361, Val CER: 0.3511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/80 [Train]: 100%|██████████| 960/960 [00:44<00:00, 21.56it/s, loss=0.6]    \n",
      "Epoch 15/80 [Val]: 100%|██████████| 54/54 [00:04<00:00, 13.37it/s, loss=1.25]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/80:\n",
      "  Train Loss: 0.4645\n",
      "  Val Loss: 0.4626, Val CER: 0.3213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/80 [Train]: 100%|██████████| 960/960 [00:45<00:00, 20.91it/s, loss=0.582]  \n",
      "Epoch 16/80 [Val]: 100%|██████████| 54/54 [00:03<00:00, 14.12it/s, loss=0.0143] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint at epoch 16 with CER: 0.3041\n",
      "Epoch 16/80:\n",
      "  Train Loss: 0.4270\n",
      "  Val Loss: 0.3784, Val CER: 0.3041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/80 [Train]: 100%|██████████| 960/960 [00:51<00:00, 18.54it/s, loss=0.358]  \n",
      "Epoch 17/80 [Val]: 100%|██████████| 54/54 [00:03<00:00, 13.81it/s, loss=0.224]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/80:\n",
      "  Train Loss: 0.4255\n",
      "  Val Loss: 0.4394, Val CER: 0.3152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/80 [Train]: 100%|██████████| 960/960 [00:52<00:00, 18.41it/s, loss=-0.0997] \n",
      "Epoch 18/80 [Val]: 100%|██████████| 54/54 [00:03<00:00, 14.13it/s, loss=0.228]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/80:\n",
      "  Train Loss: 0.3997\n",
      "  Val Loss: 0.3776, Val CER: 0.3088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/80 [Train]: 100%|██████████| 960/960 [00:50<00:00, 18.90it/s, loss=0.274]   \n",
      "Epoch 19/80 [Val]: 100%|██████████| 54/54 [00:04<00:00, 12.78it/s, loss=1.05]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/80:\n",
      "  Train Loss: 0.4138\n",
      "  Val Loss: 0.4173, Val CER: 0.3158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/80 [Train]: 100%|██████████| 960/960 [00:53<00:00, 17.91it/s, loss=0.187]   \n",
      "Epoch 20/80 [Val]: 100%|██████████| 54/54 [00:03<00:00, 14.01it/s, loss=0.357] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint at epoch 20 with CER: 0.2941\n",
      "Epoch 20/80:\n",
      "  Train Loss: 0.3752\n",
      "  Val Loss: 0.3425, Val CER: 0.2941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/80 [Train]: 100%|██████████| 960/960 [00:51<00:00, 18.50it/s, loss=0.739]    \n",
      "Epoch 21/80 [Val]: 100%|██████████| 54/54 [00:03<00:00, 13.59it/s, loss=0.998]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/80:\n",
      "  Train Loss: 0.3569\n",
      "  Val Loss: 0.7627, Val CER: 0.4112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/80 [Train]: 100%|██████████| 960/960 [00:52<00:00, 18.45it/s, loss=0.327]   \n",
      "Epoch 22/80 [Val]: 100%|██████████| 54/54 [00:03<00:00, 13.81it/s, loss=1.34]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/80:\n",
      "  Train Loss: 0.3545\n",
      "  Val Loss: 0.3719, Val CER: 0.2947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/80 [Train]: 100%|██████████| 960/960 [00:45<00:00, 21.21it/s, loss=0.581]   \n",
      "Epoch 23/80 [Val]: 100%|██████████| 54/54 [00:04<00:00, 12.94it/s, loss=1.06]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/80:\n",
      "  Train Loss: 0.3574\n",
      "  Val Loss: 0.4859, Val CER: 0.3318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/80 [Train]: 100%|██████████| 960/960 [00:45<00:00, 21.32it/s, loss=0.138]   \n",
      "Epoch 24/80 [Val]: 100%|██████████| 54/54 [00:03<00:00, 13.64it/s, loss=0.991]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/80:\n",
      "  Train Loss: 0.3350\n",
      "  Val Loss: 0.3842, Val CER: 0.2959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/80 [Train]: 100%|██████████| 960/960 [00:45<00:00, 21.11it/s, loss=0.744]   \n",
      "Epoch 25/80 [Val]: 100%|██████████| 54/54 [00:04<00:00, 13.24it/s, loss=0.546]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/80:\n",
      "  Train Loss: 0.3133\n",
      "  Val Loss: 0.3863, Val CER: 0.2968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/80 [Train]: 100%|██████████| 960/960 [00:44<00:00, 21.45it/s, loss=0.734]   \n",
      "Epoch 26/80 [Val]: 100%|██████████| 54/54 [00:04<00:00, 12.70it/s, loss=1.04]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/80:\n",
      "  Train Loss: 0.3119\n",
      "  Val Loss: 0.4360, Val CER: 0.3105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/80 [Train]: 100%|██████████| 960/960 [00:44<00:00, 21.61it/s, loss=0.232]   \n",
      "Epoch 27/80 [Val]: 100%|██████████| 54/54 [00:04<00:00, 13.11it/s, loss=0.693]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/80:\n",
      "  Train Loss: 0.2976\n",
      "  Val Loss: 0.4017, Val CER: 0.3114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/80 [Train]: 100%|██████████| 960/960 [00:44<00:00, 21.73it/s, loss=0.0434]  \n",
      "Epoch 28/80 [Val]: 100%|██████████| 54/54 [00:04<00:00, 12.98it/s, loss=0.654]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/80:\n",
      "  Train Loss: 0.2699\n",
      "  Val Loss: 0.5147, Val CER: 0.3171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/80 [Train]: 100%|██████████| 960/960 [00:44<00:00, 21.73it/s, loss=0.0356]  \n",
      "Epoch 29/80 [Val]: 100%|██████████| 54/54 [00:04<00:00, 12.44it/s, loss=0.0977] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint at epoch 29 with CER: 0.2859\n",
      "Epoch 29/80:\n",
      "  Train Loss: 0.2645\n",
      "  Val Loss: 0.3407, Val CER: 0.2859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/80 [Train]: 100%|██████████| 960/960 [00:45<00:00, 21.17it/s, loss=0.264]    \n",
      "Epoch 30/80 [Val]: 100%|██████████| 54/54 [00:04<00:00, 13.40it/s, loss=1.2]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/80:\n",
      "  Train Loss: 0.2416\n",
      "  Val Loss: 0.4066, Val CER: 0.2923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/80 [Train]: 100%|██████████| 960/960 [00:37<00:00, 25.28it/s, loss=0.243]   \n",
      "Epoch 31/80 [Val]: 100%|██████████| 54/54 [00:03<00:00, 15.72it/s, loss=0.636] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/80:\n",
      "  Train Loss: 0.2454\n",
      "  Val Loss: 0.4128, Val CER: 0.2998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/80 [Train]: 100%|██████████| 960/960 [00:37<00:00, 25.61it/s, loss=0.357]    \n",
      "Epoch 32/80 [Val]: 100%|██████████| 54/54 [00:03<00:00, 15.74it/s, loss=0.685]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/80:\n",
      "  Train Loss: 0.2224\n",
      "  Val Loss: 0.3970, Val CER: 0.3001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/80 [Train]: 100%|██████████| 960/960 [00:37<00:00, 25.59it/s, loss=0.539]   \n",
      "Epoch 33/80 [Val]: 100%|██████████| 54/54 [00:03<00:00, 15.79it/s, loss=-0.199] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/80:\n",
      "  Train Loss: 0.2115\n",
      "  Val Loss: 0.3488, Val CER: 0.2875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/80 [Train]: 100%|██████████| 960/960 [00:37<00:00, 25.63it/s, loss=0.491]   \n",
      "Epoch 34/80 [Val]: 100%|██████████| 54/54 [00:03<00:00, 15.62it/s, loss=-0.193] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/80:\n",
      "  Train Loss: 0.2115\n",
      "  Val Loss: 0.3938, Val CER: 0.2929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/80 [Train]: 100%|██████████| 960/960 [00:37<00:00, 25.49it/s, loss=0.155]    \n",
      "Epoch 35/80 [Val]: 100%|██████████| 54/54 [00:03<00:00, 16.49it/s, loss=0.24]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/80:\n",
      "  Train Loss: 0.1889\n",
      "  Val Loss: 0.4352, Val CER: 0.2949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/80 [Train]: 100%|██████████| 960/960 [00:37<00:00, 25.41it/s, loss=0.264]   \n",
      "Epoch 36/80 [Val]: 100%|██████████| 54/54 [00:03<00:00, 16.14it/s, loss=0.777]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/80:\n",
      "  Train Loss: 0.1815\n",
      "  Val Loss: 0.4146, Val CER: 0.2891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/80 [Train]: 100%|██████████| 960/960 [00:37<00:00, 25.78it/s, loss=0.128]   \n",
      "Epoch 37/80 [Val]: 100%|██████████| 54/54 [00:03<00:00, 16.36it/s, loss=0.781]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint at epoch 37 with CER: 0.2851\n",
      "Epoch 37/80:\n",
      "  Train Loss: 0.1677\n",
      "  Val Loss: 0.3859, Val CER: 0.2851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/80 [Train]: 100%|██████████| 960/960 [00:37<00:00, 25.39it/s, loss=0.248]    \n",
      "Epoch 38/80 [Val]: 100%|██████████| 54/54 [00:03<00:00, 15.74it/s, loss=0.649]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/80:\n",
      "  Train Loss: 0.1525\n",
      "  Val Loss: 0.4914, Val CER: 0.3167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/80 [Train]: 100%|██████████| 960/960 [00:37<00:00, 25.79it/s, loss=0.068]    \n",
      "Epoch 39/80 [Val]: 100%|██████████| 54/54 [00:03<00:00, 15.83it/s, loss=0.49]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/80:\n",
      "  Train Loss: 0.1475\n",
      "  Val Loss: 0.4856, Val CER: 0.3194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/80 [Train]: 100%|██████████| 960/960 [00:37<00:00, 25.72it/s, loss=0.245]   \n",
      "Epoch 40/80 [Val]: 100%|██████████| 54/54 [00:03<00:00, 15.79it/s, loss=0.679] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/80:\n",
      "  Train Loss: 0.1484\n",
      "  Val Loss: 0.3829, Val CER: 0.2893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/80 [Train]: 100%|██████████| 960/960 [00:37<00:00, 25.66it/s, loss=0.148]   \n",
      "Epoch 41/80 [Val]: 100%|██████████| 54/54 [00:03<00:00, 16.27it/s, loss=0.315] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/80:\n",
      "  Train Loss: 0.1340\n",
      "  Val Loss: 0.4342, Val CER: 0.2982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42/80 [Train]: 100%|██████████| 960/960 [00:37<00:00, 25.52it/s, loss=0.0559]   \n",
      "Epoch 42/80 [Val]: 100%|██████████| 54/54 [00:03<00:00, 16.53it/s, loss=0.705] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/80:\n",
      "  Train Loss: 0.1259\n",
      "  Val Loss: 0.6024, Val CER: 0.3078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43/80 [Train]: 100%|██████████| 960/960 [00:37<00:00, 25.63it/s, loss=0.225]    \n",
      "Epoch 43/80 [Val]: 100%|██████████| 54/54 [00:03<00:00, 16.22it/s, loss=0.661]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/80:\n",
      "  Train Loss: 0.1201\n",
      "  Val Loss: 0.4222, Val CER: 0.2957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44/80 [Train]: 100%|██████████| 960/960 [00:37<00:00, 25.29it/s, loss=0.248]    \n",
      "Epoch 44/80 [Val]: 100%|██████████| 54/54 [00:03<00:00, 16.24it/s, loss=0.764]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/80:\n",
      "  Train Loss: 0.1101\n",
      "  Val Loss: 0.4572, Val CER: 0.2889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45/80 [Train]: 100%|██████████| 960/960 [00:37<00:00, 25.33it/s, loss=0.217]    \n",
      "Epoch 45/80 [Val]: 100%|██████████| 54/54 [00:03<00:00, 16.07it/s, loss=0.598]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/80:\n",
      "  Train Loss: 0.0951\n",
      "  Val Loss: 0.5224, Val CER: 0.3063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46/80 [Train]: 100%|██████████| 960/960 [00:37<00:00, 25.32it/s, loss=0.108]    \n",
      "Epoch 46/80 [Val]: 100%|██████████| 54/54 [00:03<00:00, 16.12it/s, loss=0.515]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/80:\n",
      "  Train Loss: 0.0845\n",
      "  Val Loss: 0.4266, Val CER: 0.2912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47/80 [Train]: 100%|██████████| 960/960 [00:37<00:00, 25.40it/s, loss=-0.00688] \n",
      "Epoch 47/80 [Val]: 100%|██████████| 54/54 [00:03<00:00, 16.16it/s, loss=0.508]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint at epoch 47 with CER: 0.2834\n",
      "Epoch 47/80:\n",
      "  Train Loss: 0.0673\n",
      "  Val Loss: 0.4396, Val CER: 0.2834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48/80 [Train]: 100%|██████████| 960/960 [00:37<00:00, 25.80it/s, loss=-0.241]   \n",
      "Epoch 48/80 [Val]: 100%|██████████| 54/54 [00:03<00:00, 16.15it/s, loss=0.534]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/80:\n",
      "  Train Loss: 0.0620\n",
      "  Val Loss: 0.4628, Val CER: 0.2937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49/80 [Train]: 100%|██████████| 960/960 [00:37<00:00, 25.60it/s, loss=-0.0421]  \n",
      "Epoch 49/80 [Val]: 100%|██████████| 54/54 [00:03<00:00, 15.74it/s, loss=0.671] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/80:\n",
      "  Train Loss: 0.0550\n",
      "  Val Loss: 0.6670, Val CER: 0.3267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50/80 [Train]: 100%|██████████| 960/960 [00:37<00:00, 25.30it/s, loss=-0.157]   \n",
      "Epoch 50/80 [Val]: 100%|██████████| 54/54 [00:03<00:00, 15.52it/s, loss=0.949]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/80:\n",
      "  Train Loss: 0.0420\n",
      "  Val Loss: 0.5311, Val CER: 0.3034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 51/80 [Train]: 100%|██████████| 960/960 [00:37<00:00, 25.40it/s, loss=-0.269]   \n",
      "Epoch 51/80 [Val]: 100%|██████████| 54/54 [00:03<00:00, 15.70it/s, loss=1.03]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/80:\n",
      "  Train Loss: 0.0326\n",
      "  Val Loss: 0.4859, Val CER: 0.2893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 52/80 [Train]: 100%|██████████| 960/960 [00:37<00:00, 25.49it/s, loss=0.119]   \n",
      "Epoch 52/80 [Val]: 100%|██████████| 54/54 [00:03<00:00, 15.79it/s, loss=0.369] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint at epoch 52 with CER: 0.2808\n",
      "Epoch 52/80:\n",
      "  Train Loss: 0.0257\n",
      "  Val Loss: 0.4462, Val CER: 0.2808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 53/80 [Train]: 100%|██████████| 960/960 [00:37<00:00, 25.51it/s, loss=-0.283]   \n",
      "Epoch 53/80 [Val]: 100%|██████████| 54/54 [00:03<00:00, 15.69it/s, loss=1.35]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/80:\n",
      "  Train Loss: 0.0242\n",
      "  Val Loss: 0.5439, Val CER: 0.3016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 54/80 [Train]: 100%|██████████| 960/960 [00:37<00:00, 25.57it/s, loss=0.324]   \n",
      "Epoch 54/80 [Val]: 100%|██████████| 54/54 [00:03<00:00, 16.31it/s, loss=0.965]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/80:\n",
      "  Train Loss: 0.0145\n",
      "  Val Loss: 0.5226, Val CER: 0.2911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 55/80 [Train]: 100%|██████████| 960/960 [00:37<00:00, 25.83it/s, loss=-0.152]   \n",
      "Epoch 55/80 [Val]: 100%|██████████| 54/54 [00:03<00:00, 16.12it/s, loss=0.957]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/80:\n",
      "  Train Loss: 0.0008\n",
      "  Val Loss: 0.4709, Val CER: 0.2812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 56/80 [Train]: 100%|██████████| 960/960 [00:37<00:00, 25.79it/s, loss=0.573]    \n",
      "Epoch 56/80 [Val]: 100%|██████████| 54/54 [00:03<00:00, 16.32it/s, loss=1.17]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/80:\n",
      "  Train Loss: -0.0010\n",
      "  Val Loss: 0.5180, Val CER: 0.3002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 57/80 [Train]: 100%|██████████| 960/960 [00:37<00:00, 25.69it/s, loss=-0.093]   \n",
      "Epoch 57/80 [Val]: 100%|██████████| 54/54 [00:03<00:00, 16.21it/s, loss=0.877]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/80:\n",
      "  Train Loss: -0.0039\n",
      "  Val Loss: 0.4906, Val CER: 0.2897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 58/80 [Train]: 100%|██████████| 960/960 [00:37<00:00, 25.83it/s, loss=0.37]     \n",
      "Epoch 58/80 [Val]: 100%|██████████| 54/54 [00:03<00:00, 16.52it/s, loss=0.924] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/80:\n",
      "  Train Loss: -0.0176\n",
      "  Val Loss: 0.5497, Val CER: 0.3012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 59/80 [Train]: 100%|██████████| 960/960 [00:36<00:00, 25.97it/s, loss=-0.121]   \n",
      "Epoch 59/80 [Val]: 100%|██████████| 54/54 [00:03<00:00, 16.36it/s, loss=0.918]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/80:\n",
      "  Train Loss: -0.0219\n",
      "  Val Loss: 0.5838, Val CER: 0.2949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 60/80 [Train]: 100%|██████████| 960/960 [00:37<00:00, 25.65it/s, loss=0.439]    \n",
      "Epoch 60/80 [Val]: 100%|██████████| 54/54 [00:03<00:00, 16.07it/s, loss=1.08]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/80:\n",
      "  Train Loss: -0.0327\n",
      "  Val Loss: 0.5539, Val CER: 0.2896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 61/80 [Train]: 100%|██████████| 960/960 [00:37<00:00, 25.58it/s, loss=-0.0414]  \n",
      "Epoch 61/80 [Val]: 100%|██████████| 54/54 [00:03<00:00, 16.44it/s, loss=1.1]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/80:\n",
      "  Train Loss: -0.0356\n",
      "  Val Loss: 0.6184, Val CER: 0.3182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 62/80 [Train]: 100%|██████████| 960/960 [00:37<00:00, 25.79it/s, loss=0.167]    \n",
      "Epoch 62/80 [Val]: 100%|██████████| 54/54 [00:03<00:00, 16.28it/s, loss=1.15]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/80:\n",
      "  Train Loss: -0.0371\n",
      "  Val Loss: 0.7488, Val CER: 0.3393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 63/80 [Train]: 100%|██████████| 960/960 [00:37<00:00, 25.77it/s, loss=-0.155]   \n",
      "Epoch 63/80 [Val]: 100%|██████████| 54/54 [00:03<00:00, 16.35it/s, loss=1.14]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63/80:\n",
      "  Train Loss: -0.0457\n",
      "  Val Loss: 0.7533, Val CER: 0.3319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 64/80 [Train]: 100%|██████████| 960/960 [00:37<00:00, 25.84it/s, loss=-0.113]   \n",
      "Epoch 64/80 [Val]: 100%|██████████| 54/54 [00:03<00:00, 16.42it/s, loss=1.14]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64/80:\n",
      "  Train Loss: -0.0469\n",
      "  Val Loss: 0.5488, Val CER: 0.2890\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 65/80 [Train]: 100%|██████████| 960/960 [00:36<00:00, 26.07it/s, loss=-0.239]   \n",
      "Epoch 65/80 [Val]: 100%|██████████| 54/54 [00:03<00:00, 16.41it/s, loss=1.07]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65/80:\n",
      "  Train Loss: -0.0564\n",
      "  Val Loss: 0.5966, Val CER: 0.2998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 66/80 [Train]: 100%|██████████| 960/960 [00:37<00:00, 25.72it/s, loss=0.21]     \n",
      "Epoch 66/80 [Val]: 100%|██████████| 54/54 [00:03<00:00, 16.21it/s, loss=1.01]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66/80:\n",
      "  Train Loss: -0.0560\n",
      "  Val Loss: 0.5658, Val CER: 0.2967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 67/80 [Train]: 100%|██████████| 960/960 [00:37<00:00, 25.64it/s, loss=0.0902]   \n",
      "Epoch 67/80 [Val]: 100%|██████████| 54/54 [00:03<00:00, 16.41it/s, loss=1.2]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67/80:\n",
      "  Train Loss: -0.0591\n",
      "  Val Loss: 0.5701, Val CER: 0.3015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 68/80 [Train]: 100%|██████████| 960/960 [00:37<00:00, 25.90it/s, loss=0.0505]  \n",
      "Epoch 68/80 [Val]: 100%|██████████| 54/54 [00:03<00:00, 16.49it/s, loss=1.08]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68/80:\n",
      "  Train Loss: -0.0649\n",
      "  Val Loss: 0.6791, Val CER: 0.3179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 69/80 [Train]: 100%|██████████| 960/960 [00:36<00:00, 25.99it/s, loss=-0.0969]  \n",
      "Epoch 69/80 [Val]: 100%|██████████| 54/54 [00:03<00:00, 16.35it/s, loss=1.34]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69/80:\n",
      "  Train Loss: -0.0672\n",
      "  Val Loss: 0.6248, Val CER: 0.3078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 70/80 [Train]: 100%|██████████| 960/960 [00:37<00:00, 25.58it/s, loss=-0.0979]  \n",
      "Epoch 70/80 [Val]: 100%|██████████| 54/54 [00:03<00:00, 16.53it/s, loss=1.33]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70/80:\n",
      "  Train Loss: -0.0689\n",
      "  Val Loss: 0.6871, Val CER: 0.3182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 71/80 [Train]: 100%|██████████| 960/960 [00:37<00:00, 25.92it/s, loss=0.0619]   \n",
      "Epoch 71/80 [Val]: 100%|██████████| 54/54 [00:03<00:00, 15.85it/s, loss=1.22]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/80:\n",
      "  Train Loss: -0.0735\n",
      "  Val Loss: 0.6612, Val CER: 0.3123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 72/80 [Train]: 100%|██████████| 960/960 [00:37<00:00, 25.32it/s, loss=-0.0671]  \n",
      "Epoch 72/80 [Val]: 100%|██████████| 54/54 [00:03<00:00, 15.68it/s, loss=1.21]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/80:\n",
      "  Train Loss: -0.0753\n",
      "  Val Loss: 0.5852, Val CER: 0.2980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 73/80 [Train]: 100%|██████████| 960/960 [00:37<00:00, 25.32it/s, loss=0.0493]   \n",
      "Epoch 73/80 [Val]: 100%|██████████| 54/54 [00:03<00:00, 15.87it/s, loss=1.14]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/80:\n",
      "  Train Loss: -0.0776\n",
      "  Val Loss: 0.6715, Val CER: 0.3111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 74/80 [Train]: 100%|██████████| 960/960 [00:37<00:00, 25.29it/s, loss=0.0624]   \n",
      "Epoch 74/80 [Val]: 100%|██████████| 54/54 [00:03<00:00, 15.70it/s, loss=1.06]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/80:\n",
      "  Train Loss: -0.0780\n",
      "  Val Loss: 0.6359, Val CER: 0.3078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 75/80 [Train]: 100%|██████████| 960/960 [00:37<00:00, 25.30it/s, loss=0.00388]  \n",
      "Epoch 75/80 [Val]: 100%|██████████| 54/54 [00:03<00:00, 15.83it/s, loss=1.01]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75/80:\n",
      "  Train Loss: -0.0829\n",
      "  Val Loss: 0.6769, Val CER: 0.3167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 76/80 [Train]: 100%|██████████| 960/960 [00:37<00:00, 25.46it/s, loss=-0.26]    \n",
      "Epoch 76/80 [Val]: 100%|██████████| 54/54 [00:03<00:00, 15.18it/s, loss=1.05]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/80:\n",
      "  Train Loss: -0.0799\n",
      "  Val Loss: 0.6892, Val CER: 0.3192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 77/80 [Train]: 100%|██████████| 960/960 [00:37<00:00, 25.64it/s, loss=-0.00821] \n",
      "Epoch 77/80 [Val]: 100%|██████████| 54/54 [00:03<00:00, 15.71it/s, loss=1.05]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/80:\n",
      "  Train Loss: -0.0822\n",
      "  Val Loss: 0.6351, Val CER: 0.3079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 78/80 [Train]: 100%|██████████| 960/960 [00:37<00:00, 25.45it/s, loss=-0.158]   \n",
      "Epoch 78/80 [Val]: 100%|██████████| 54/54 [00:03<00:00, 15.79it/s, loss=1.04]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/80:\n",
      "  Train Loss: -0.0855\n",
      "  Val Loss: 0.6621, Val CER: 0.3115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 79/80 [Train]: 100%|██████████| 960/960 [00:38<00:00, 25.23it/s, loss=-0.186]   \n",
      "Epoch 79/80 [Val]: 100%|██████████| 54/54 [00:03<00:00, 15.65it/s, loss=1.08]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/80:\n",
      "  Train Loss: -0.0809\n",
      "  Val Loss: 0.6278, Val CER: 0.3064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 80/80 [Train]: 100%|██████████| 960/960 [00:37<00:00, 25.36it/s, loss=0.134]    \n",
      "Epoch 80/80 [Val]: 100%|██████████| 54/54 [00:03<00:00, 15.80it/s, loss=1.11]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/80:\n",
      "  Train Loss: -0.0845\n",
      "  Val Loss: 0.6782, Val CER: 0.3125\n",
      "Best validation CER: 0.2808 at epoch 52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize the EMGConformer model with better parameters\n",
    "model = EMGConformer(\n",
    "    num_classes=99,\n",
    "    d_model=256,        # Increased from 128 for better representation\n",
    "    nhead=8,            # Increased from 4 for more attention heads\n",
    "    num_layers=4,       # Increased from 2 for deeper model\n",
    "    dropout=0.3         # Reduced from 0.5 to prevent overfitting\n",
    ")\n",
    "\n",
    "# Print model architecture and parameter count\n",
    "print(model)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "\n",
    "# Move the model to the device\n",
    "model = model.to(device)\n",
    "\n",
    "# Test with a small batch\n",
    "try:\n",
    "    print(\"Testing model with a small batch...\")\n",
    "    # Get a small batch\n",
    "    test_batch = next(iter(train_loader))\n",
    "    test_inputs = test_batch['inputs'].to(device)\n",
    "    test_lengths = test_batch['input_lengths'].to(device)\n",
    "    \n",
    "    # Test forward pass\n",
    "    with torch.no_grad():\n",
    "        outputs = model(test_inputs, test_lengths)\n",
    "    \n",
    "    print(f\"Forward pass successful! Output shape: {outputs.shape}\")\n",
    "    \n",
    "    # Now try the training\n",
    "    print(\"Starting training...\")\n",
    "    history, best_epoch, best_val_cer = train_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        device=device,\n",
    "        epochs=80,      # Train for more epochs\n",
    "        lr=0.001,        # Better learning rate\n",
    "        weight_decay=1e-4  # Slightly reduced weight decay\n",
    "    )\n",
    "    print(f\"Best validation CER: {best_val_cer:.4f} at epoch {best_epoch}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error encountered: {type(e).__name__}: {e}\")\n",
    "    \n",
    "    # Try with small batch only for debugging\n",
    "    if 'small_train_loader' in globals() and 'small_val_loader' in globals():\n",
    "        print(\"Trying with small data loaders for debugging...\")\n",
    "        try:\n",
    "            history = train_model(\n",
    "                model=model,\n",
    "                train_loader=small_train_loader,\n",
    "                val_loader=small_val_loader,\n",
    "                device=device,\n",
    "                epochs=2,  # Just a few epochs\n",
    "                lr=0.001,\n",
    "                weight_decay=1e-4\n",
    "            )\n",
    "        except Exception as e2:\n",
    "            print(f\"Error with small data loaders: {type(e2).__name__}: {e2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using results from epoch 59\n",
      "Number of epochs with predictions: 80\n",
      "Number of samples in epoch 59: 1698\n",
      "Sample 12 prediction: [1, 17, 14, 22, 7, 96, 5, 14, 23, 96]\n",
      "Number of targets in epoch 59: 1698\n",
      "Sample 12 target: [ 1 17 14 22 13 96  5 14 23 96]\n",
      "prediction: browh fox \n",
      "target    : brown fox \n",
      "\n",
      "Additional examples:\n",
      "Example 1:\n",
      "  prediction: \n",
      "  target    : \n",
      "Example 2:\n",
      "  prediction: \n",
      "  target    : \n",
      "Example 3:\n",
      "  prediction: \n",
      "  target    : \n",
      "Example 4:\n",
      "  prediction: \n",
      "  target    : \n",
      "Example 5:\n",
      "  prediction: \n",
      "  target    : \n"
     ]
    }
   ],
   "source": [
    "all_predictions_lst = history['all_predictions']\n",
    "all_targets_lst = history['all_targets']\n",
    "\n",
    "# Make sure this epoch exists in your history\n",
    "epoch_id = min(60-1, len(all_predictions_lst)-1)  # Get the last epoch by default\n",
    "print(f\"Using results from epoch {epoch_id}\")\n",
    "\n",
    "# Output how many epochs of predictions we have\n",
    "print(f\"Number of epochs with predictions: {len(all_predictions_lst)}\")\n",
    "\n",
    "# Make sure we have predictions for this epoch\n",
    "if epoch_id < len(all_predictions_lst) and all_predictions_lst[epoch_id]:\n",
    "    # Get number of samples and choose one that exists\n",
    "    num_samples = len(all_predictions_lst[epoch_id])\n",
    "    it = min(12, num_samples-1)  # Default to sample 12 or the last one if fewer\n",
    "    \n",
    "    print(f\"Number of samples in epoch {epoch_id}: {num_samples}\")\n",
    "    print(f\"Sample {it} prediction: {all_predictions_lst[epoch_id][it]}\")\n",
    "    pred = all_predictions_lst[epoch_id][it]\n",
    "    \n",
    "    print(f\"Number of targets in epoch {epoch_id}: {len(all_targets_lst[epoch_id])}\")\n",
    "    print(f\"Sample {it} target: {all_targets_lst[epoch_id][it]}\")\n",
    "    target = all_targets_lst[epoch_id][it]\n",
    "    \n",
    "    # Convert indices to characters\n",
    "    char_set = charset()\n",
    "    \n",
    "    # Filter out null class tokens and duplicates from predictions\n",
    "    filtered_pred = []\n",
    "    prev = None\n",
    "    for p in pred:\n",
    "        if p != char_set.null_class and p != prev:\n",
    "            filtered_pred.append(p)\n",
    "        prev = p\n",
    "    \n",
    "    pred_chars = char_set.labels_to_str(filtered_pred)\n",
    "    target_chars = char_set.labels_to_str(target)\n",
    "    print(f'prediction: {pred_chars}')\n",
    "    print(f'target    : {target_chars}')\n",
    "    \n",
    "    # Show additional examples\n",
    "    print(\"\\nAdditional examples:\")\n",
    "    for i in range(5):\n",
    "        sample_idx = min(i, num_samples-1)\n",
    "        pred = all_predictions_lst[epoch_id][sample_idx]\n",
    "        target = all_targets_lst[epoch_id][sample_idx]\n",
    "        \n",
    "        # Filter out null class tokens and duplicates\n",
    "        filtered_pred = []\n",
    "        prev = None\n",
    "        for p in pred:\n",
    "            if p != char_set.null_class and p != prev:\n",
    "                filtered_pred.append(p)\n",
    "            prev = p\n",
    "            \n",
    "        pred_chars = char_set.labels_to_str(filtered_pred)\n",
    "        target_chars = char_set.labels_to_str(target)\n",
    "        print(f'Example {i+1}:')\n",
    "        print(f'  prediction: {pred_chars}')\n",
    "        print(f'  target    : {target_chars}')\n",
    "else:\n",
    "    print(f\"No predictions available for epoch {epoch_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading best model from epoch 52...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Evaluation: 100%|██████████| 71/71 [00:03<00:00, 17.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test CER: 0.3325\n",
      "\n",
      "Test Examples:\n",
      "Example 1:\n",
      "  prediction: \n",
      "  target    : \n",
      "Example 2:\n",
      "  prediction: \n",
      "  target    : \n",
      "Example 3:\n",
      "  prediction: \n",
      "  target    : \n",
      "Example 4:\n",
      "  prediction: \n",
      "  target    : \n",
      "Example 5:\n",
      "  prediction: \n",
      "  target    : \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def evaluate_on_test(model, test_loader, device):\n",
    "    \"\"\"Evaluate the model on test data and compute CER\"\"\"\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(test_loader, desc=\"Test Evaluation\")\n",
    "        for batch in progress_bar:\n",
    "            inputs = batch['inputs'].to(device)\n",
    "            targets = batch['targets'].to(device)\n",
    "            input_lengths = batch['input_lengths'].to(device)\n",
    "            target_lengths = batch['target_lengths'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs, input_lengths)\n",
    "            \n",
    "            # Compute log probabilities\n",
    "            log_probs = outputs.log_softmax(2)\n",
    "            \n",
    "            # Adjust input lengths for downsampling in the model\n",
    "            input_lengths = torch.div(input_lengths, 2, rounding_mode='floor')\n",
    "            input_lengths = torch.clamp(input_lengths, min=1)\n",
    "            \n",
    "            # Decode predictions\n",
    "            predictions = decode_predictions(log_probs.cpu(), input_lengths.cpu())\n",
    "            targets_np = targets.cpu().numpy()\n",
    "            \n",
    "            # Store predictions and targets for CER calculation\n",
    "            for i in range(inputs.size(1)):\n",
    "                target_seq = targets_np[:target_lengths[i].item(), i]\n",
    "                all_predictions.append(predictions[i])\n",
    "                all_targets.append(target_seq)\n",
    "    \n",
    "    # Calculate Character Error Rate\n",
    "    cer = calculate_cer(all_predictions, all_targets)\n",
    "    \n",
    "    return cer, all_predictions, all_targets\n",
    "\n",
    "# Load the best model\n",
    "print(f\"Loading best model from epoch {best_epoch}...\")\n",
    "best_model = EMGConformer(\n",
    "    num_classes=99,\n",
    "    d_model=256,\n",
    "    nhead=8,\n",
    "    num_layers=4,\n",
    "    dropout=0.3\n",
    ").to(device)\n",
    "best_model.load_state_dict(torch.load('conformer_best.pth'))\n",
    "\n",
    "# Evaluate on test data\n",
    "test_cer, test_predictions, test_targets = evaluate_on_test(best_model, test_loader, device)\n",
    "print(f\"Test CER: {test_cer:.4f}\")\n",
    "\n",
    "# Optional: Display a few examples\n",
    "char_set = charset()\n",
    "print(\"\\nTest Examples:\")\n",
    "for i in range(min(5, len(test_predictions))):\n",
    "    pred = test_predictions[i]\n",
    "    target = test_targets[i]\n",
    "    \n",
    "    # Filter out null class tokens and duplicates\n",
    "    filtered_pred = []\n",
    "    prev = None\n",
    "    for p in pred:\n",
    "        if p != char_set.null_class and p != prev:\n",
    "            filtered_pred.append(p)\n",
    "        prev = p\n",
    "    \n",
    "    pred_chars = char_set.labels_to_str(filtered_pred)\n",
    "    target_chars = char_set.labels_to_str(target)\n",
    "    print(f'Example {i+1}:')\n",
    "    print(f'  prediction: {pred_chars}')\n",
    "    print(f'  target    : {target_chars}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''class SimpleEMGConformer(nn.Module):\n",
    "    def __init__(self, num_classes=99, d_model=128, nhead=4, num_layers=2, dropout=0.5):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Input EMG signal dimensions\n",
    "        self.features = 1056  # 2 bands * 16 channels * 33 frequencies\n",
    "        \n",
    "        # Enhanced CNN feature extraction\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv1d(self.features, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            \n",
    "            nn.Conv1d(256, d_model, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm1d(d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),  # Only one pooling layer\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "        \n",
    "        # Simple Transformer encoder layers instead of full Conformer blocks\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=d_model*2,\n",
    "            dropout=dropout,\n",
    "            batch_first=False\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, input_lengths=None):\n",
    "        # x shape: [T, N, B, C, F] - [time, batch, bands, channels, freq]\n",
    "        T, N, B, C, F = x.shape\n",
    "        device = x.device\n",
    "        \n",
    "        # Flatten features and prepare for Conv1d\n",
    "        x = x.reshape(T, N, B*C*F).permute(1, 2, 0)  # [N, B*C*F, T]\n",
    "        \n",
    "        # Apply CNN layers\n",
    "        x = self.conv_layers(x)  # [N, d_model, T//2]\n",
    "        \n",
    "        # Calculate new sequence lengths after CNN pooling (only one pooling layer)\n",
    "        if input_lengths is not None:\n",
    "            new_lengths = torch.div(input_lengths, 2, rounding_mode='floor')\n",
    "            new_lengths = torch.clamp(new_lengths, min=1)\n",
    "            # Create padding mask for transformer\n",
    "            max_len = x.size(2)\n",
    "            padding_mask = (torch.arange(max_len, device=device).expand(N, max_len) \n",
    "                           >= new_lengths.unsqueeze(1))\n",
    "        else:\n",
    "            padding_mask = None\n",
    "        \n",
    "        # Prepare for transformer: [T//2, N, d_model]\n",
    "        x = x.permute(2, 0, 1)\n",
    "        \n",
    "        # Apply positional encoding\n",
    "        x = self.pos_encoder(x)\n",
    "        \n",
    "        # Apply transformer encoder\n",
    "        x = self.transformer_encoder(x, src_key_padding_mask=padding_mask)\n",
    "        \n",
    "        # Apply classifier\n",
    "        time_steps, batch_size, hidden_dim = x.size()\n",
    "        x = x.reshape(-1, hidden_dim)\n",
    "        x = self.classifier(x)\n",
    "        x = x.view(time_steps, batch_size, -1)\n",
    "        \n",
    "        return x'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
